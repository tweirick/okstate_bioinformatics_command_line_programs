'''
run_svm_learn.py
@author: Tyler Weirick
@Created on: 6/18/2012 Version 0.0 
@language:Python 3.2
@tags: svm train 

This program handles the 5-fold training and validating process. 
It takes a 

/home/TWeirick/COMBINED_FASTAS_6.7.12/40per_6102012/40per_6102012_fastas/NON_REDUNDANT_FASTAS/40_100_40_FASTAS/FIVE_FOLDER_TRAINING_VECTORS

python3 /home/TWeirick/PY_PROGRAMS/PYSVM/multiclass_five_fold_test_classes.py --help


'''








from multiclass_five_fold_test_functions import *
from sys import exit
from glob import glob
import os
import subprocess
from time import time
from math import *
import tempfile
from random import gauss


def getheadcomments():
    """
    This function will make a string from the text between the first and 
    second ''' encountered. Its purpose is to make maintenance of the comments
    easier by only requiring one change for the main comments. 
    """
    desc_list = []
    start_and_break = "'''"
    read_line_bool = False
    #Get self name and read self line by line. 
    for line in open(__file__,'r'):
        if read_line_bool:
            if not start_and_break in line:
                line_minus_newline = line.replace("\n","")
                space_list = []
                #Add spaces to lines less than 79 chars
                for i in range(len(line_minus_newline),80):
                     space_list.append(" ")
                desc_list.append(line_minus_newline+''.join(space_list)+"\n\r")
            else:
                break    
        if (start_and_break in line) and read_line_bool == False:
            read_line_bool = True
    desc = ''.join(desc_list)
    return desc


try:
    import argparse
    def getargs(ver='%prog 0.0'):
        parser = argparse.ArgumentParser(description=getheadcomments())    
        parser.add_argument('--file_set',help='')
        parser.add_argument('--k' ,
                            default=5,
                            help='')
        args = parser.parse_args()
        sorted_file_glob = sorted(glob(args.file_set))
        use_vector_strings = args.k
        return sorted_file_glob,use_vector_strings
except:
    try:
        import optparse
    except:
        print("ERROR 1: Cannot import argparse or optparse")



class FeaturePoint():
    """
    This class describes a point generated by some type of feature(s).
    It holds a string describing the point as well as the name of the 
    class the point is from and can return the point as a positive 
    negative or named point. 
    """
    def __init__(self,feature_line):
        assert type(feature_line) == str
        split_line = line.split()
        #Make sure that a name exists.
        #@todo: make a regex to recognize proper svm format.
        assert not ":" in split_line[0],"ERROR: Point done not have name"         
        self.example_type       = split_line[0]
        self.true_class_name    = split_line[0]
        self.vector_coordiantes = " ".join(split_line[1:])
        self.predicted_class    = None
        self.value_of_pred      = None
    
    def getPositivepoint(self):
        return "+1 "+self.vector_coordiantes
    def getNegativepoint(self):
        return "-1 "+self.vector_coordiantes
    def getnamedpoint(self):
        return self.example_type+" "+self.vector_coordiantes
    
    ##class SVMClassification():
    
    #def __init__(self,entry_class):
    #    #self.pos_set_class_name = pos_set_class_name
    #    self.true_class_name   = entry_class
    #    self.predicted_class   = None
    #    self.value_of_pred     = None
    
    def updateprediction(self,class_name,value):
        assert type(class_name) == str
        assert type(value)      == float 
        if (self.predicted_class == None and self.value_of_pred == None):
            self.predicted_class   = class_name
            self.value_of_pred     = value 
        else:
           if value > self.value_of_pred:  
               self.predicted_class   = class_name
               self.value_of_pred     = value 
           elif value == self.value_of_pred:
               print("WARNING: values for",self.predicted_class,self.true_class_name,
               "and",class_name,"are equal. It could be likely your",
               "classes have repeats or overlapping sequences.")
           
           
           
    def checkprediction(self,checking_class):
        '''
        if (class_ID == prot_class):
            total_class_seqs+=1
            if (prot_class == class_of_largest_value):
                #True Positive + +)
                #print("True Positive")
                class_calcs_dict[prot_class]["TP"]+=1 
            else:# + and -
                class_calcs_dict[prot_class]["FN"]+=1 
        else:# (class_ID != prot_class):
            if prot_class == class_of_largest_value:# - + 
                class_calcs_dict[prot_class]["FP"]+=1
            else: # - -
                class_calcs_dict[prot_class]["TN"]+=1
        '''
        if checking_class == self.true_class_name:
            if checking_class == self.predicted_class:
                return "TP"
            else:#if self.predicted_class != self.true_class_name:
                return "FN"
        else:#hecking_class != self.true_class_name:
            if checking_class == self.predicted_class:
                return "FP"
            else:#if self.predicted_class != self.true_class_name:
                return "TN"
            

class ClassificationSet():
    """
    This class represents one class within the classification system.
    """
    def __init__(self,class_name,k):
        self.class_name = class_name
        self.FeaturePoint_class_list = []
        self.k = k
        self.remainder      = 0
        self.average_length = 0
        
    def addclasspointlist(self,class_points_list):
        #This should be a complete list of all members of a class.
        #assert type(class_points_list) == list, class_points_list
        self.remainder = len(class_points_list)%self.k
        self.average_length = int(len(class_points_list)/self.k)
        self.FeaturePoint_class_list = class_points_list

    def getsubset(self,i):
        """
        Returns a subset 1/k of the total FeaturePoints in the FeaturePoint 
        class list. 
        """
        start = i*self.average_length
        stop  = start+self.average_length
        if self.remainder > 0:
            stop+=1
            self.remainder-=1
        return self.FeaturePoint_class_list[start:stop]   

    def gettextsubset(self,i):
        """
        Returns a subset 1/k of the total FeaturePoints in the FeaturePoint 
        class list. 
        """
        start = i*self.average_length
        stop  = start+self.average_length
        if self.remainder > 0:
            stop+=1
            self.remainder-=1
        out_list = []
        for e in self.FeaturePoint_class_list[start:stop]:
            out_list.append(e.getnamedpoint())
        return "\n".join(out_list)


#Make training sets
#If negative training data present add here.
#Make k-fold training sets.

class SVMLearnClass():
    
    def __init__(self,class_name,pos_neg_examples_str,test_examples):
        self.pos_examples_are_class = class_name
        self.pos_neg_list  = pos_neg_examples_str
        self.test_examples = test_examples_str
        


    
class KFoldSet():
    def __init__(self):
        self.general_example_data = []
        self.test_data            = []


#=============================================================================
DEBUG = True    
sorted_file_glob,k = getargs()
#classification_set = ClassificationSet(k)
#kfoldsets = 
#if DEBUG: print(nfoldsets)

class TrainingFile():
    def __init__(self,pcn,tt):
        self.protein_class_name = ""
        self.training_text = ""
        

class_name_collision_check_list = []
#Make a list of ClassificationSets, each ClassificationSet will have one of the 
#classes of data desired 
feature_ClassificationSet_list = []
for featurized_file_with_ID in sorted_file_glob:
     #Get class name.
     class_name = featurized_file_with_ID.split(".")[0]
     feature_ClassificationSet = ClassificationSet(class_name,k)
     
     if class_name in class_name_collision_check_list:
          print("ERROR Overlapping class names found:",class_name)
          exit()
     class_name_collision_check_list.append(class_name)
     
     #Add the data points in the file to ClassificationSet
     example_list = []
     for line in open(featurized_file_with_ID,"r"):
        example_list.append(FeaturePoint(line))
     feature_ClassificationSet.addclasspointlist(example_list)
     #This list contains all ClassificationSet objects.
     #These should all contain the same training IDs.
     feature_ClassificationSet_list.append(feature_ClassificationSet)
     print(feature_ClassificationSet.class_name,
           len(feature_ClassificationSet.FeaturePoint_class_list))
     


class Predictions():    
    def __init__(self):
        self.test_file_2D_list = []     


#Make a list containing k elements containing ~ k-1/k of the total data.
k_fold_learning_set = []
for k_level in range(0,k):
    general_example_data = []
    test_data            = []
    calc_data            = []
    for j in range(0,k):
        if k_level == j:
            for classifer in feature_ClassificationSet_list:
                #Make a list of FeaturePoint Classes 
                test_data.append(classifer.gettextsubset(k_level))
                '''
                SVMClassification():
    
    def __init__(self,entry_class):
                '''
                calc_data = calc_data + classifer.getsubset(k_level)
        else:
            for classifer in feature_ClassificationSet_list:
                general_example_data = general_example_data+classifer.getsubset(k_level)
                
    k_fold_learning_set.append([general_example_data,"\n".join(test_data),calc_data])
    

class PlusMinusFile():
    def __init__(self,psn,training_data):
        self.pos_set_name  = psn
        self.training_data = training_data
        #self.test_file     = test_file ,test_fil

#Should be K of these
class KFold():
    
    def __init__(self,pmdl,tf,cd):
        self.plus_minus_data_list = pmdl
        assert type(tf) == str
        self.test_file = tf
        self.calc_data = cd



k_fold_list = []
k_level_plus_minus_list = []

print(len(k_fold_learning_set))

for i in range(0,k):
    fold_set = []
    for k_fold in k_fold_learning_set:
        training_data,test_data,calc_data = k_fold

        plus_minus_data_list = []
        for class_name in class_name_collision_check_list:
            #for n class_types make plus negative files.
            ex = []
            for example in training_data:
                if example.example_type == class_name:
                    ex.append(example.getPositivepoint())
                else:
                    ex.append(example.getNegativepoint())
            #Should be 12 
            plus_minus_data_list.append(PlusMinusFile(class_name,"\n".join(ex)))
        fold_set.append(plus_minus_data_list)

    k_fold_list.append(KFold(plus_minus_data_list,test_data,calc_data))
    #k_level_plus_minus_list.append(fold_set)


def dofoldclassification(c_cond,j_cond,g_cond,
                         training_data,test_file_name,prediction_file_name):
    #Make the training file. 
    training_file = tempfile.NamedTemporaryFile(mode='w')  
    training_file.write(training_data)
    training_file.flush()
    
    #Make the model data into this file.
    model_file = tempfile.NamedTemporaryFile(mode='r')        

    #Run SVM learn svm_learn    example1/train.dat example1/model        
    subprocess.call("./svm_learn -z c -t 1 "+
     " -c "+c_cond+" -j "+j_cond+" -g "+g_cond+
     " "+training_file.name+" "+model_file.name,
     shell=True) 
    #Training data is no longer needed as we now have a model file. Will be deleted on close.
    training_file.close()
        
    #svm_classify example1/test.dat example1/model example1/predictions
    subprocess.call("./svm_classify "+test_file_name+" "+model_file.name+
    " "+predictions_file.name,shell=True) 
    
    #Model file no longer needed as we have predictions. 
    model_file.close()

class PerformanceCalculation():
    
    def __init__(self,FN,FP,TN,TP):
        self.FN = FN
        self.FP = FP
        self.TP = TP
        self.TN = TN
        
    def getaccuracy(self):
        numerator   = 100*(self.TP+self.TN)
        denominator = (self.TP+self.TN+self.FP+self.FN)
        return numerator/denominator
            
    def getprecision(self):
        numerator   = 100*self.TP
        denominator = self.TP+self.FP
        return numerator/denominator
        
    def getsensitivity(self):
        #{"Sensitivity":100*true_pos/(true_pos+false_neg)})
        numerator   = 100*self.TP
        denominator = self.TP+self.FN
        return numerator/denominator
        
    def getspecificity(self):
        #{"Specificity":100*true_neg/(true_neg+false_pos)})
        numerator   = 100*self.TN
        denominator = self.TN+self.FP
        return numerator/denominator
    
    def getMCC(self):
        #numerator = (true_pos*true_neg)-(false_pos*false_neg)
        #denominator = (true_pos+false_pos)*(true_pos+false_neg)*(true_neg+false_pos)*(true_neg+false_neg) 
        numerator   = self.TP*self.TN-self.FP*self.FN
        denominator = sqrt((self.TP+self.FP)*(self.TP+self.FN)*(self.TN+self.FP)*(self.TN+self.FN))
        return numerator/denominator
    
    def geterror(self):
        #({"Error": 100*numerator/denominator })  numerator   = 100*TP
        #numerator   = (false_pos+false_neg)
        #denominator = (true_pos+true_neg+false_pos+false_neg)
        numerator   = 100*self.FP+self.FN
        denominator = (self.TP+self.TN+self.FP+self.FN)
        return numerator/denominator


def testAt():
    c_cond = "50"
    j_cond = "2"
    g_cond = "600"
    return c_cond,j_cond,g_cond


def sample_gaussian(mu,sigma,N):
    assert type(mu) == list
    assert len(mu) > 0  
    assert type(N) == int
    assert N > 0
    N_out_points = []
    for i in range(0,N):
        new_point = []
        for j in range(0,mu): 
            new_point.append(gauss(mu[j],sigma))
        N_out_points.append(new_point)
    return N_out_points


def meanof2Dlist(twoD_list):
    some_list = []
    cnt = 0
    for i in range(0,twoD_list-1):
        for j in range(0,len(point)):
            twoD_list[0][j]+=twoD_list[i+1][j]
        cnt+=1
    for i in range(0,len(twoD_list[0])):
        twoD_list[0][i] = twoD_list[0][i]/cnt
        
    return twoD_list[0]

def vectorsubtract(a,b):
     assert len(a) == len(b)
     out_vec = []
     for i in range(0,len(a)):
         out_vec.append(a[i] - b[i])
     return out_vec

def vectoraddition(a,b):
     assert len(a) == len(b)
     out_vec = []
     for i in range(0,len(a)):
         out_vec.append(a[i] + b[i])
     return out_vec

def vectormultiply(a,b):
     assert len(a) == len(b)
     out_vec = []
     for i in range(0,len(a)):
         out_vec.append(a[i] * b[i])
     return out_vec

def two_pass_2D_variance(data):
    assert len(data) > 0
    assert type(data[0]) == list
    n  = len(data)
    #sum1 = 0
    sum2 = 0
    #for x in data:
    #    n    = n + 1
    #    sum1 = sum1 + x
    #mean = sum1/n
    mean = meanof2Dlist(data)
    for x in data:
        x_minus_mean = vectorsubtract(x,mean)
        #sum2 = sum2 + (x - mean)*(x - mean)
        x_minus_mean2 = vectormultiply(x_minus_mean,x_minus_mean)
        sum2 = vectoraddition(sum2,x_minus_mean2)
    
    variance = []
    for e in sum2:
        variance.append( e/(n - 1) )
        
    return variance
    




def makeconfusionmatrix(results_list,class_name_collision_check_list):

    confusion_matrix = {}
    for e in results_list:
        print(type(e))
        comb = e.true_class_name+" "+e.predicted_class
        if comb in confusion_matrix:
            confusion_matrix[comb]+=1
        else:
            confusion_matrix.update({comb:1})
            
    horizontal_title = ["Class"]
    rows = []
    for class1 in sorted(class_name_collision_check_list):
        horizontal_title.append("Predicted_"+class1)
        other_stuff = ["Actual_"+class1]
        for class2 in sorted(class_name_collision_check_list):
            if class1+" "+class2 in confusion_matrix:
                numb_str = str(confusion_matrix[class1+" "+class2])
            else:
                numb_str = "0"    
            other_stuff.append(class1+"-"+class2+":"+numb_str)
        rows.append(" ".join(other_stuff))
    
    return(" ".join(horizontal_title) +"\n"+"\n".join(rows))
    
    

def calculateOverallMCC(results_list,class_name_collision_check_list):
    
    osd = {"TP":0,"TN":0,"FP":0,"FN":0}
    #Calculate Statistics need per class stats as well as overall.
    for class_name in class_name_collision_check_list:
        total=0
        for e in results_list:
            if e.true_class_name == class_name:
                total+=1     
            osd[e.checkprediction(class_name)]+=1    
    print(total,pc.getMCC())

def optimize_c_j_g_with_NCE(pred_qual_measure_list):
    N = len(pred_qual_measure_list)
    epsilon=1^e-4
    top_n_percent = 0.4
    
    if sigma2 > epsilon:#t < maxits and maxits = 100
        keep = ceil(int(len(pred_qual_measure_list)*top_n_percent))
        #Get the top top_n_percent of the results.
        top_values_list = (
            sorted(pred_qual_measure_list,reverse=True)[0:keep])
        #Make a list of only the points.
        points_list = []
        for preform_value in top_values_list:
            points_list.append(preform_value[1])
        #Calculate a new mu and sigma for the gaussian distribution.
        mu     = meanof2Dlist(points_list)
        sigma2 = two_pass_2D_variance(points_list)
        #Return N new points to check.
        return sample_gaussian(mu,sigma2,N)
    else:
        return False 
             
def make_initial_RBF_point_list(c_start,c_end,j_start,j_end,g_start,g_end):
     parts = 3
     out_point_list = []
     c_increment = int( (c_end-c_start)/parts)
     for c in range(c_start,c_end,c_increment):
         j_increment = int( (j_end-j_start)/parts)
         for j in range(j_start,j_end,j_increment):
             g_increment = int( (g_end-g_start)/parts)
             for g in range(g_start,g_end,g_increment):
                 out_point_list.append([c,j,g])
                 print([c,j,g])
     return out_point_list
 
c_start,c_end = 0,500
j_start,j_end = 0,15
g_start,g_end = 0,15

point_list = make_initial_RBF_point_list(c_start,c_end,j_start,j_end,g_start,g_end)
max_mcc = 0.0

while point_list:
    for point in point_list:
        c_cond,j_cond,g_cond = point
        results_list = []    
        for k_fold in k_fold_list:#1-5
            #Get fold testing data.
            test_str = k_fold.test_file
            #List of feature points
            calc_data = k_fold.calc_data
            assert test_str.count('\n')+1 == len(calc_data)
            #Make temporary file to hold testing data. 
            test_file = tempfile.NamedTemporaryFile(mode='w')
            test_file.write(test_str)
            for plus_minus_class in k_fold.plus_minus_data_list:#12
                i = 0
                #Make predictions file, one float per line. 
                predictions_file = tempfile.NamedTemporaryFile(mode='r')
                #Do svm_learn and svm_classify results written into prediction_file.name
                dofoldclassification(c_cond,j_cond,g_cond,
                    plus_minus_class.training_data,test_file.name,
                    predictions_file.name)
                for line in open(predictions_file.name,'r'):
                    #There will be one floating point number for each line. 
                    calc_data[i].updateprediction(
                    postive_set_class_name,float(line.strip()));i+=1
                predictions_file.close()
            results_list = results_list + calc_data
            
            #================================================
            #Test file no longer needed as we have predictions. 
            test_file.close()
        overall_MCC = calculateOverallMCC(results_list)
        overall_MCC_list = [overall_MCC,point]
        if max_mcc < overall_MCC:
            #Write detailed stats to a file
            #Vector Type
            #Time
            #Conditions
            file = open("training_best_stats.txt",'w')
            file.write()
            file.close()
            #Vector Type
            #Time
            #Conditions
            file = open("best_stats_confusion_matrix.txt",'w')
            file.write(makeconfusionmatrix(results_list,class_name_collision_check_list))
            file.close()
            
        #append run history to logfile. 
        #Time conditions MCC
        file = open("training_history.txt",'a')
        file.write()
        file.close()
    point_list = optimize_c_j_g_with_NCE(overall_MCC_list)
    




        



    

"""
    print("Classification took",time()-t1,"seconds")
    
    overall_MCC
     
    #If overall MCC higher than previous.
    if overall_MCC > highest_MCC:
        #Make Confusion Matrix
        makeconfusionmatrix()
    
    #Return a list of mcc values, and the c,j,g linked to them 
    return 
    

    
#Output overall MCC to log file.
for e in results_list:
    #
    print("sdfasdfasdf")
    #count_instances 
    
for e in osd:
    print(e,osd[e])
        #Add to predictions_class 
        #This will calculate and hold the largest value input into 
        #the class for each testing entry
        
        #Input these prediction classes into a function which will calculate over all stats 
        #And update the best entry's confusion matrix and stats, for all others simply keep 
        #The final overall calculations
        #now we should have a matrix 
        #keep for the best run and make confusion matrix 
        
        
def optimize_c_j_g_with_NCE(mu,sigma_squared,t,maxits,N,Ne):
    #[x,count,sz]
    epsilon=1^e-4
    while t < maxits and sigma2 > epsilon:
        N_points_list = sample_gaussian(mu,sigma2,N); 
        S_list = []
        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!11!!!!
        for point in N_points_list:
            S_list.append([function_result(c,j,g),point,])
        #Sort in desending order
        X = sorted(S_list)
        mu = meanof2Dlist(X[0:Ne]); 
        #sigma2 = var(X(1:Ne));
        sigma2 = two_pass_2D_variance(X[1:Ne])
        t = t+1;     
"""
        



        
        
    


